{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyJ4NMmxnH6O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Lambda, MaxPooling2D, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML5p_EFrnxNi"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "BUCKETS = 5\n",
        "MAX_ANGLE = 450 # May need to confirm this\n",
        "one_hot_length = int(900/BUCKETS)\n",
        "BINS = np.arange(0, MAX_ANGLE - (MAX_ANGLE % BUCKETS) + 2*BUCKETS, BUCKETS)\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 64 # 128, 64, 32, 8, 6\n",
        "DATA_VALUE = 'X'\n",
        "Y_VALUE_1 = 'speed'\n",
        "Y_VALUE_2 = 'steering_angle'\n",
        "timesteps = 100 # 64, 5\n",
        "interval = 10\n",
        "shift_amount = 10\n",
        "number_outputs = int(timesteps/interval)\n",
        "img_shape = (128, 128, 3) #(100, 100, 3) # (3, 160, 320), # \n",
        "predicting_frame_num = 3\n",
        "error_margin = 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZfwF21GAPTw"
      },
      "source": [
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK7-kVMTAL1X"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne40l0t7Zgxk"
      },
      "source": [
        "# Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDvkz1DuDBKy",
        "outputId": "1473db52-7805-4ba3-d28d-48fc5130bd48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32.03473518\n"
          ]
        }
      ],
      "source": [
        "train_files = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-06-08--11-46-01.h5', # 2.64\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-01-30--11-24-51.h5', # 7.62\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-01-30--13-46-00.h5', # 8.5\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-02-02--10-16-58.h5', # 8.06\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-03-29--10-50-20.h5',  # 11.28 GB\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-02-11--21-32-47.h5'] # 12.3 GB\n",
        "\n",
        "train_labels = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-06-08--11-46-01_NEW.h5',\n",
        "                '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-01-30--11-24-51_NEW.h5',\n",
        "                '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-01-30--13-46-00_NEW.h5', \n",
        "                '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-02-02--10-16-58_NEW.h5', \n",
        "                '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-03-29--10-50-20_NEW.h5', \n",
        "                '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-02-11--21-32-47_NEW.h5']\n",
        "\n",
        "train_small_imgs = train_files[0:2]\n",
        "train_medium_imgs = train_files[0:4]\n",
        "train_large_imgs = train_files[:]\n",
        "\n",
        "train_tfRecord_path_s = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), train_small_imgs))\n",
        "train_tfRecord_path_m = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), train_medium_imgs))\n",
        "train_tfRecord_path_l = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), train_large_imgs))\n",
        "\n",
        "train_new_files = glob.glob('/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/comma2k19TF/train/*')\n",
        "train_pick = int(len(train_new_files)*.45)\n",
        "train_new_files = random.choices(train_new_files, k=train_pick)\n",
        "\n",
        "size = 0\n",
        "for item in train_new_files:\n",
        "  size += os.path.getsize(item)\n",
        "print(size/1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8spsZpx_d-Ih",
        "outputId": "a4ceb599-e308-4638-e9dc-f2b5ea83c8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.101766556\n"
          ]
        }
      ],
      "source": [
        "val_files = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-01-31--19-19-25.h5', # 2.93\n",
        "             '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-05-12--22-20-00.h5', # 7.47 GB\n",
        "             '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-02-08--14-56-28.h5'] # 3.81\n",
        "\n",
        "val_labels = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-01-31--19-19-25_NEW.h5',\n",
        "              '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-05-12--22-20-00_NEW.h5', \n",
        "              '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-02-08--14-56-28_NEW.h5']\n",
        "\n",
        "validation_path_imgs_s = val_files[0]\n",
        "validation_path_imgs_m = val_files[0:2]\n",
        "validation_path_imgs_l = val_files[:]\n",
        "\n",
        "val_tfRecord_path_s = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), validation_path_imgs_s))\n",
        "val_tfRecord_path_m = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), validation_path_imgs_m))\n",
        "val_tfRecord_path_l = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), validation_path_imgs_l))\n",
        "\n",
        "val_new_files = glob.glob('/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/comma2k19TF/val/*')\n",
        "val_pick = int(len(val_new_files)*.35)\n",
        "val_new_files = random.choices(val_new_files, k=val_pick)\n",
        "\n",
        "size = 0\n",
        "for item in val_new_files:\n",
        "  size += os.path.getsize(item)\n",
        "print(size/1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU3vm-QmepB3"
      },
      "outputs": [],
      "source": [
        "test_files = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-04-21--14-48-08.h5', # 4.39\n",
        "              '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/camera/2016-06-02--21-39-29.h5'] # 6.45\n",
        "\n",
        "test_labels = ['/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-04-21--14-48-08_NEW.h5',\n",
        "               '/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/log/2016-06-02--21-39-29_NEW.h5']\n",
        "\n",
        "testing_path_imgs_s = test_files[0]\n",
        "testing_path_imgs_ml = test_files[:]\n",
        "\n",
        "test_tfRecord_path_s = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), testing_path_imgs_s))\n",
        "test_tfRecord_path_ml = list(map(lambda x: x.replace('camera', 'TFRecords/Data_2').replace('.h5', '.tfrecord'), testing_path_imgs_ml))\n",
        "\n",
        "test_new_files = glob.glob('/content/drive/Shareddrives/ELEC 494 - Ω2Ω/Data/comma2k19TF/test/*')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JIE2WcfDZcyt"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dujaCD-ZOGhl"
      },
      "source": [
        "## Previous generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch0gt8HzYKlc"
      },
      "outputs": [],
      "source": [
        "class generator():\n",
        "\n",
        "    def __init__(self, method):\n",
        "      \n",
        "      assert method in ['bucketize', 'normal', 'only_angle', 'bucketize_angle'], \"Method should be the following: 'bucketize', 'normal', 'only_angle', 'bucketize_angle'\"\n",
        "\n",
        "      if method == 'bucketize':\n",
        "        self.output = self.bucketize\n",
        "\n",
        "      elif method == 'normal':\n",
        "        self.output = self.normal\n",
        "      else:\n",
        "        self.output = self.only_angle\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_features(example_proto):\n",
        "\n",
        "      features = {'X': tf.io.FixedLenFeature([], dtype=tf.string), \n",
        "                  'steering_angle': tf.io.FixedLenFeature([], dtype=tf.float32), \n",
        "                  'speed': tf.io.FixedLenFeature([], dtype=tf.float32),}\n",
        "\n",
        "      parsed_features = tf.io.parse_single_example(example_proto, features)\n",
        "      feat_dtypes = [tf.float32, tf.string, tf.int64]\n",
        "\n",
        "      data_types = {\"steering_angle\": tf.float32, \"speed\": tf.float32, \"X\": tf.uint8,}\n",
        "                    \n",
        "      image = tf.io.decode_raw(parsed_features['X'], tf.uint8)\n",
        "      image = tf.reshape(image, shape=img_shape)\n",
        "      speed = parsed_features['speed']\n",
        "      angle = parsed_features['steering_angle']\n",
        "\n",
        "      return image, (angle, speed)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_inteval(x, outputs):\n",
        "\n",
        "      angle, speed = outputs\n",
        "      return x.batch(timesteps).get_single_element()[:-1:interval], (angle.batch(timesteps).get_single_element()[-1], speed.batch(timesteps).get_single_element()[-1])\n",
        "\n",
        "    def bucketize(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        if angle < 0:\n",
        "          pos_neg = 1\n",
        "        else:\n",
        "          pos_neg = 0\n",
        "        degree_bucketized = np.zeros((90,))\n",
        "        indx = sum(BINS<np.abs(angle))-1\n",
        "        degree_bucketized[indx] = 1\n",
        "\n",
        "        yield(img, {'sign': pos_neg, 'magnitude': degree_bucketized, 'speed': speed})\n",
        "\n",
        "    def bucketize_angle(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        angle_bucketized = (sum(BINS<np.abs(angle))-1) * np.round(angle/(np.abs(angle)+1e-8)) # 11 -> 1, 5 -> 0, -5 -> 0, -11 -> -1, \n",
        "        yield(img, angle_bucketized)\n",
        "\n",
        "    def bucketize_angle_one_hot(self, img, angle, speed):\n",
        "      \n",
        "      if angle <= MAX_ANGLE:\n",
        "\n",
        "        new_MAX_ANGLE = 2 * MAX_ANGLE\n",
        "        BINS_2 = np.arange(0, new_MAX_ANGLE - (new_MAX_ANGLE % BUCKETS) + 2*BUCKETS, BUCKETS)\n",
        "        vector_lenght = int(new_MAX_ANGLE/BUCKETS)\n",
        "        degree_bucketized = np.zeros((vector_lenght,))\n",
        "\n",
        "        pos_angle = angle + MAX_ANGLE # Convert all angles to positive\n",
        "        idx = (sum(BINS_2<np.abs(pos_angle))-1)\n",
        "\n",
        "        degree_bucketized[idx] = 1\n",
        "        yield(img, degree_bucketized)\n",
        "\n",
        "    def normal(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        yield(img, {'angle': angle, 'speed': speed})\n",
        "\n",
        "    def only_angle(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        yield(img, angle) # /MAX_ANGLE\n",
        "\n",
        "    def __call__(self, tfRecord):\n",
        "\n",
        "      dataset = tf.data.TFRecordDataset(tfRecord)\n",
        "      dataset = dataset.map(generator._parse_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      dataset = dataset.skip(tf.random.uniform(shape=(), minval=0, maxval=shift_amount, dtype=tf.int64))\n",
        "      dataset = dataset.window(timesteps, shift=shift_amount, drop_remainder=True)\n",
        "      dataset = dataset.map(generator.get_inteval)\n",
        "\n",
        "      for img, (angle, speed) in dataset:\n",
        "        # if angle <= MAX_ANGLE:\n",
        "        #   yield(img, angle) # /MAX_ANGLE\n",
        "        if angle <= MAX_ANGLE:\n",
        "          angle_bucketized = (sum(BINS<np.abs(angle))-1) * np.round(angle/(np.abs(angle)+1e-8)) # 11 -> 1, 5 -> 0, -5 -> 0, -11 -> -1, \n",
        "          yield(img, angle_bucketized)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsNBL5HOKmi"
      },
      "source": [
        "## new generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4EMjf9FN6cA"
      },
      "outputs": [],
      "source": [
        "class generator():\n",
        "\n",
        "    def __init__(self, method):\n",
        "      \n",
        "      assert method in ['bucketize', 'normal', 'only_angle', 'bucketize_angle', \n",
        "                        'one_hot_bucket_angle', '1_hot_angle_wSpeed'], \"Method should be the following: 'bucketize', 'normal', 'only_angle', 'bucketize_angle', 'one_hot_bucket_angle', '1_hot_angle_wSpeed'\"\n",
        "\n",
        "      # This idea doesnt work :(\n",
        "      if method == 'bucketize':\n",
        "        self.output = self.bucketize\n",
        "\n",
        "      elif method == 'normal':\n",
        "        self.output = self.normal\n",
        "      else:\n",
        "        self.output = self.only_angle\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_features(example_proto):\n",
        "\n",
        "      features = {'X': tf.io.FixedLenFeature([], dtype=tf.string), \n",
        "                  'steering_angle': tf.io.FixedLenFeature([], dtype=tf.float32), \n",
        "                  'speed': tf.io.FixedLenFeature([], dtype=tf.float32),}\n",
        "\n",
        "      parsed_features = tf.io.parse_single_example(example_proto, features)\n",
        "      feat_dtypes = [tf.float32, tf.string, tf.int64]\n",
        "\n",
        "      data_types = {\"steering_angle\": tf.float32, \"speed\": tf.float32, \"X\": tf.uint8,}\n",
        "                    \n",
        "      image = tf.io.decode_raw(parsed_features['X'], tf.uint8)\n",
        "      image = tf.reshape(image, shape=img_shape)\n",
        "      speed = parsed_features['speed']\n",
        "      angle = parsed_features['steering_angle']\n",
        "\n",
        "      return image, (angle, speed)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_inteval(x, outputs):\n",
        "\n",
        "      angle, speed = outputs\n",
        "      return x.batch(timesteps).get_single_element()[:-1:interval], (angle.batch(timesteps).get_single_element()[-1], speed.batch(timesteps).get_single_element()[-1])\n",
        "\n",
        "    def bucketize(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        if angle < 0:\n",
        "          pos_neg = 1\n",
        "        else:\n",
        "          pos_neg = 0\n",
        "        degree_bucketized = np.zeros((90,))\n",
        "        indx = sum(BINS<np.abs(angle))-1\n",
        "        degree_bucketized[indx] = 1\n",
        "\n",
        "        yield(img, {'sign': pos_neg, 'magnitude': degree_bucketized, 'speed': speed})\n",
        "\n",
        "    def bucketize_angle(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        angle_bucketized = (sum(BINS<np.abs(angle))-1) * np.round(angle/(np.abs(angle)+1e-8)) # 11 -> 1, 5 -> 0, -5 -> 0, -11 -> -1, \n",
        "        yield(img, angle_bucketized)\n",
        "\n",
        "    def bucketize_angle_one_hot(self, img, angle, speed):\n",
        "      \n",
        "      if angle <= MAX_ANGLE:\n",
        "\n",
        "        new_MAX_ANGLE = 2 * MAX_ANGLE\n",
        "        BINS_2 = np.arange(0, new_MAX_ANGLE - (new_MAX_ANGLE % BUCKETS) + 2*BUCKETS, BUCKETS)\n",
        "        vector_lenght = int(new_MAX_ANGLE/BUCKETS)\n",
        "        degree_bucketized = np.zeros((vector_lenght,))\n",
        "\n",
        "        pos_angle = angle + MAX_ANGLE # Convert all angles to positive\n",
        "        idx = (sum(BINS_2<np.abs(pos_angle))-1)\n",
        "\n",
        "        degree_bucketized[idx] = 1\n",
        "        yield(img, degree_bucketized)\n",
        "\n",
        "    def normal(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        yield(img, {'angle': angle, 'speed': speed})\n",
        "\n",
        "    def only_angle(self, img, angle, speed):\n",
        "\n",
        "      if angle <= MAX_ANGLE:\n",
        "        yield(img, angle) # /MAX_ANGLE\n",
        "\n",
        "    def __call__(self, tfRecord):\n",
        "\n",
        "      dataset = tf.data.TFRecordDataset(tfRecord)\n",
        "      dataset = dataset.map(generator._parse_features, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      dataset = dataset.skip(tf.random.uniform(shape=(), minval=0, maxval=shift_amount, dtype=tf.int64))\n",
        "      dataset = dataset.window(timesteps, shift=shift_amount, drop_remainder=True)\n",
        "      dataset = dataset.map(generator.get_inteval)\n",
        "\n",
        "      new_MAX_ANGLE = 2 * MAX_ANGLE\n",
        "      BINS_2 = np.arange(0, new_MAX_ANGLE - (new_MAX_ANGLE % BUCKETS), BUCKETS)\n",
        "      vector_lenght = int(new_MAX_ANGLE/BUCKETS)\n",
        "\n",
        "      for img, (angle, speed) in dataset:\n",
        "        if angle <= MAX_ANGLE:\n",
        "          degree_bucketized = np.zeros((vector_lenght,))\n",
        "          pos_angle = angle + MAX_ANGLE # Convert all angles to positive\n",
        "          idx = (sum(BINS_2<=np.abs(pos_angle))-1)\n",
        "          degree_bucketized[idx] = 1\n",
        "          yield(img, {'angle': degree_bucketized, 'speed': speed})\n",
        "\n",
        "          # if idx >= error_margin and idx <= vector_lenght-1 - error_margin:\n",
        "          #   degree_bucketized[idx - error_margin:idx] = 1\n",
        "          #   degree_bucketized[idx: idx + 1 + error_margin] = 1\n",
        "          #   yield(img, {'angle': degree_bucketized, 'speed': speed})\n",
        "\n",
        "          # if idx < error_margin:\n",
        "          #   degree_bucketized[:idx] = 1\n",
        "          #   degree_bucketized[idx: idx + 1 + error_margin] = 1\n",
        "          #   yield(img, {'angle': degree_bucketized, 'speed': speed})\n",
        "\n",
        "          # if idx > vector_lenght-1 - error_margin:\n",
        "          #   degree_bucketized[idx - error_margin:idx] = 1\n",
        "          #   degree_bucketized[idx:] = 1\n",
        "            \n",
        "          #   yield(img, {'angle': degree_bucketized, 'speed': speed})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9YO2ligQayM"
      },
      "outputs": [],
      "source": [
        "method = 'bucketize_angle'\n",
        "if method == 'normal': output_signature = (tf.TensorSpec(shape=(number_outputs,) + img_shape, dtype=tf.uint8, name=None), {'angle': tf.TensorSpec(shape=(), dtype=tf.float64, name=None), 'speed': tf.TensorSpec(shape=(), dtype=tf.float64, name=None)})\n",
        "if method == 'bucketize': output_signature = (tf.TensorSpec(shape=(number_outputs,) + img_shape, dtype=tf.uint8, name=None), {'sign': tf.TensorSpec(shape=(), dtype=tf.float64, name=None), 'magnitude': tf.TensorSpec(shape=(90,), dtype=tf.float64, name=None), 'speed': tf.TensorSpec(shape=(), dtype=tf.float64, name=None)})\n",
        "if method == 'only_angle': output_signature = (tf.TensorSpec(shape=(number_outputs,) + img_shape, dtype=tf.uint8, name=None), tf.TensorSpec(shape=(), dtype=tf.float64, name=None))\n",
        "if method == 'bucketize_angle': output_signature = (tf.TensorSpec(shape=(number_outputs,) + img_shape, dtype=tf.uint8, name=None), tf.TensorSpec(shape=(), dtype=tf.float64, name=None))\n",
        "if method == 'one_hot_bucket_angle': output_signature = (tf.TensorSpec(shape=(number_outputs,) + img_shape, dtype=tf.uint8, name=None), tf.TensorSpec(shape=(90,), dtype=tf.float64, name=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzIIMEXYYj3B"
      },
      "outputs": [],
      "source": [
        "ds_train = tf.data.Dataset.from_tensor_slices(train_new_files)\n",
        "ds_train = ds_train.interleave(lambda filename: tf.data.Dataset.from_generator(generator(method=method),\n",
        "                                                                               output_signature=output_signature,\n",
        "                                                                               args=[filename]),\n",
        "                               cycle_length=30, \n",
        "                               num_parallel_calls = AUTOTUNE).cache()\n",
        "\n",
        "ds_train = ds_train.shuffle(20000)\n",
        "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "ds_train = ds_train.prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJcudhQMY7zH"
      },
      "outputs": [],
      "source": [
        "ds_val = tf.data.Dataset.from_tensor_slices(val_new_files)\n",
        "ds_val = ds_val.interleave(lambda filename: tf.data.Dataset.from_generator(generator(method=method),\n",
        "                                                                           output_signature=output_signature,\n",
        "                                                                           args=[filename]),\n",
        "                           cycle_length=20, \n",
        "                           num_parallel_calls = AUTOTUNE).cache()\n",
        "\n",
        "ds_val = ds_val.shuffle(20000)\n",
        "ds_val = ds_val.batch(BATCH_SIZE, drop_remainder=True)\n",
        "ds_val = ds_val.prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kF0AZE3oX_u"
      },
      "outputs": [],
      "source": [
        "ds_test = tf.data.Dataset.from_tensor_slices(test_new_files)\n",
        "ds_test = ds_test.interleave(lambda filename: tf.data.Dataset.from_generator(generator(method=method),\n",
        "                                                                   output_signature=output_signature,\n",
        "                                                                   args=[filename]),\n",
        "                   cycle_length=30, \n",
        "                   num_parallel_calls = AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.prefetch(AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6lZFY00rCnF3"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-omeM9deFW3-"
      },
      "outputs": [],
      "source": [
        "# Import Model\n",
        "# Compile with optimizer, loss and metrics\n",
        "# Initiate Callbacks\n",
        "# .fit(...)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Attempting E2E.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
